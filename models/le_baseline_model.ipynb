{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Model and Baseline Model\n",
    "\n",
    "In this notebook we are going to create a random model which returns the target label just based in the average % of class in the complete dataset. \n",
    "\n",
    "We will then build a Baseline Model by just feeding the inital pre-processed data (without further transformation) by building with that dataset a Logistic REgression, Random Forest and LightGBM model. \n",
    "\n",
    "The intention of this exercise is to be able to compare the final model to this ones, to check if the further transformations and method lead into a statistically significant increase in the accuracy and stability or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Set the working directory\n",
    "os.chdir('c:/Users/laura/OneDrive/Documentos/Personal Documents/Universidad/DSE CCNY/Courses Semester 2/Applied ML/Final Project/machine-learning-dse-i210-final-project-credit-risk/notebooks')\n",
    "# Set data paths\n",
    "data_dir = 'c:/Users/laura/OneDrive/Documentos/Personal Documents/Universidad/DSE CCNY/Courses Semester 2/Applied ML/Final Project/new_aggs/new_aggs/'\n",
    "data_base = 'C:/Users/laura/OneDrive/Documentos/Personal Documents/Universidad/DSE CCNY/Courses Semester 2/Applied ML/Project_final/home-credit-credit-risk-model-stability/csv_files/train/train_base.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "For this baseline model we will use the union of the personal and non personal data after begin preprocessed. In this stage we will use just a set of that. This same code will be later run again with the complete dataset to get the final scores on the complete data. For the model based on the percentage, we pull the percentage of positive classes from the entire dataset 3% \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .pkl file for the personal data\n",
    "file1 = data_dir + 'df1.pkl'\n",
    "df = pd.read_pickle(file1)\n",
    "\n",
    "# Load the .pkl file for the non personal data\n",
    "file2 = data_dir + 'df2.pkl'\n",
    "df2 = pd.read_pickle(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joing the applicant personal data and the non personal one to get the complete dataset.\n",
    "df_full = df.merge(df2, on=['case_id', 'date_decision', 'WEEK_NUM'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2640)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm the dataset was properly merge by cheking the shape\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>date_decision</th>\n",
       "      <th>MONTH_x</th>\n",
       "      <th>WEEK_NUM</th>\n",
       "      <th>target_x</th>\n",
       "      <th>empls_employedfrom_796D_distinct_x</th>\n",
       "      <th>empls_employedfrom_796D_min_year_x</th>\n",
       "      <th>empls_employedfrom_796D_min_month_x</th>\n",
       "      <th>empls_employedfrom_796D_min_day_x</th>\n",
       "      <th>empls_employedfrom_796D_max_year_x</th>\n",
       "      <th>...</th>\n",
       "      <th>pmts_pmtsoverdue_635A_median_y</th>\n",
       "      <th>pmts_dpdvalue_108P_sum_y</th>\n",
       "      <th>pmts_pmtsoverdue_635A_sum_y</th>\n",
       "      <th>pmts_date_1107D_distinct_y</th>\n",
       "      <th>pmts_date_1107D_min_year_y</th>\n",
       "      <th>pmts_date_1107D_min_month_y</th>\n",
       "      <th>pmts_date_1107D_min_day_y</th>\n",
       "      <th>pmts_date_1107D_max_year_y</th>\n",
       "      <th>pmts_date_1107D_max_month_y</th>\n",
       "      <th>pmts_date_1107D_max_day_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1488310</td>\n",
       "      <td>2019-08-14</td>\n",
       "      <td>201908</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13904</td>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>201905</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>783503</td>\n",
       "      <td>2019-08-28</td>\n",
       "      <td>201908</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17986</td>\n",
       "      <td>2019-06-09</td>\n",
       "      <td>201906</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1400855</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>201906</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2639 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id date_decision  MONTH_x  WEEK_NUM  target_x  \\\n",
       "0  1488310    2019-08-14   201908        32         0   \n",
       "1    13904    2019-05-06   201905        17         0   \n",
       "2   783503    2019-08-28   201908        34         1   \n",
       "3    17986    2019-06-09   201906        22         1   \n",
       "4  1400855    2019-06-13   201906        23         0   \n",
       "\n",
       "   empls_employedfrom_796D_distinct_x  empls_employedfrom_796D_min_year_x  \\\n",
       "0                                 1.0                                 NaN   \n",
       "1                                 1.0                                 NaN   \n",
       "2                                 1.0                                 NaN   \n",
       "3                                 1.0                                 NaN   \n",
       "4                                 1.0                                 NaN   \n",
       "\n",
       "   empls_employedfrom_796D_min_month_x  empls_employedfrom_796D_min_day_x  \\\n",
       "0                                  NaN                                NaN   \n",
       "1                                  NaN                                NaN   \n",
       "2                                  NaN                                NaN   \n",
       "3                                  NaN                                NaN   \n",
       "4                                  NaN                                NaN   \n",
       "\n",
       "   empls_employedfrom_796D_max_year_x  ...  pmts_pmtsoverdue_635A_median_y  \\\n",
       "0                                 NaN  ...                             NaN   \n",
       "1                                 NaN  ...                             NaN   \n",
       "2                                 NaN  ...                             NaN   \n",
       "3                                 NaN  ...                             NaN   \n",
       "4                                 NaN  ...                             NaN   \n",
       "\n",
       "   pmts_dpdvalue_108P_sum_y  pmts_pmtsoverdue_635A_sum_y  \\\n",
       "0                       NaN                          NaN   \n",
       "1                       NaN                          NaN   \n",
       "2                       NaN                          NaN   \n",
       "3                       NaN                          NaN   \n",
       "4                       NaN                          NaN   \n",
       "\n",
       "   pmts_date_1107D_distinct_y  pmts_date_1107D_min_year_y  \\\n",
       "0                         NaN                         NaN   \n",
       "1                         NaN                         NaN   \n",
       "2                         NaN                         NaN   \n",
       "3                         NaN                         NaN   \n",
       "4                         NaN                         NaN   \n",
       "\n",
       "   pmts_date_1107D_min_month_y  pmts_date_1107D_min_day_y  \\\n",
       "0                          NaN                        NaN   \n",
       "1                          NaN                        NaN   \n",
       "2                          NaN                        NaN   \n",
       "3                          NaN                        NaN   \n",
       "4                          NaN                        NaN   \n",
       "\n",
       "   pmts_date_1107D_max_year_y  pmts_date_1107D_max_month_y  \\\n",
       "0                         NaN                          NaN   \n",
       "1                         NaN                          NaN   \n",
       "2                         NaN                          NaN   \n",
       "3                         NaN                          NaN   \n",
       "4                         NaN                          NaN   \n",
       "\n",
       "   pmts_date_1107D_max_day_y  \n",
       "0                        NaN  \n",
       "1                        NaN  \n",
       "2                        NaN  \n",
       "3                        NaN  \n",
       "4                        NaN  \n",
       "\n",
       "[5 rows x 2639 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the extra target column that appeared because of the join and it is just a duplicate of the target\n",
    "df_full.rename(columns={'target_x': 'target'}, inplace=True)\n",
    "\n",
    "# Drop target_y\n",
    "df_full.drop(columns=['target_y'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Data Cleaning\n",
    "\n",
    "We futher clean the data to be able to feed the previously stated predictors.\n",
    " - Boolean Columns:\n",
    "The boolean columns are filled with False where None is present, and then converted to boolean type using .astype(bool).\n",
    "- Object Columns:\n",
    "The object columns, which contain None, True, or False, are replaced with np.nan, 1.0, or 0.0 respectively, and converted to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date_decision to timestamp\n",
    "df_full[\"date_decision\"] = pd.to_datetime(df_full[\"date_decision\"]).astype('int64') / 10**9\n",
    "\n",
    "# Get boolean columns from df_full\n",
    "bool_columns = df_full.select_dtypes(include=['bool']).columns.tolist()\n",
    "\n",
    "for col in bool_columns:\n",
    "    df_full[col] = df_full[col].fillna(False).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get object columns from df_full\n",
    "object_columns = df_full.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "for col in object_columns:\n",
    "    df_full[col] = df_full[col].replace({None: np.nan, True: 1.0, False: 0.0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Validation Split. \n",
    "\n",
    "We remove the target from the training datasets. We then further split the dataset into train and validation. For this split we do not apply startify to balanace the target, we are building the baseline which will not consider this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (7000, 2635), y_train: (7000,)\n",
      "Valid: (3000, 2635), y_valid: (3000,)\n"
     ]
    }
   ],
   "source": [
    "# Train and Validation split\n",
    "# Train and Validation split\n",
    "base = df_full[[\"case_id\", \"WEEK_NUM\", \"target\"]]\n",
    "X = df_full.drop(columns=[\"case_id\", \"WEEK_NUM\", \"target\"])\n",
    "y = df_full[\"target\"]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.7, random_state=1, stratify=y)\n",
    "\n",
    "# Prepare base_train and base_valid\n",
    "base_train = base.iloc[X_train.index]\n",
    "base_valid = base.iloc[X_valid.index]\n",
    "\n",
    "print(f\"Train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Valid: {X_valid.shape}, y_valid: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Model Predictor based on target percentage - Weighted Random Chance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of target = 1: 3.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of target = 1 in df_full\n",
    "percentage_target_1 = 3\n",
    "print(f\"Percentage of target = 1: {percentage_target_1:.2f}%\")\n",
    "\n",
    "\n",
    "def model_percentage(data, percentage):\n",
    "    random_numbers = np.random.rand(len(data))\n",
    "    predictions = (random_numbers < (percentage / 100)).astype(int)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hardcode the target positive class average to 3% which comes from the complete dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5032\n",
      "Precision: 0.1553\n",
      "Recall: 0.0399\n",
      "Accuracy: 0.8427\n",
      "F1 Score: 0.0635\n"
     ]
    }
   ],
   "source": [
    "# Apply the model to base_valid\n",
    "y_valid_pred = model_percentage(base_valid, percentage_target_1)\n",
    "\n",
    "# Evaluate the model\n",
    "auc_score = roc_auc_score(y_valid, y_valid_pred)\n",
    "precision = precision_score(y_valid, y_valid_pred)\n",
    "recall = recall_score(y_valid, y_valid_pred)\n",
    "accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "f1 = f1_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"AUC: {auc_score:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score reflects the model's ability to distinguish between classes. A score closer to 0.5 indicates that the model performs no better than random guessing. Given that this is a random model based on the target's percentage, an AUC score near 0.5 is expected and confirms the model's lack of discriminatory power.\n",
    "\n",
    "Precision measures the proportion of true positives among all positive predictions. For a random model, precision is typically low as it doesn't have any mechanism to prioritize true positives over false positives.\n",
    "\n",
    "Recall measures the proportion of actual positives that are correctly identified by the model. A random model's recall is generally proportional to the actual prevalence of the positive class in the dataset. \n",
    "\n",
    "Accuracy measures the proportion of all correct predictions (both true positives and true negatives) out of the total predictions. For a random model, the accuracy is influenced by the class distribution in the dataset. An accuracy score that is close to the proportion of the majority class indicates that the model might be leaning towards predicting the majority class more often.\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall, providing a balance between the two. We see for this model that is low, reflecting the trade-off between precision and recall when there is no specific pattern in the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "A more sofisticated Baseline we could use would be to train a Logistic Regressor, Decision Tree Classifier or LGBM Classier on their respective default parameters, which are pre-tuned to work reasonably well for a wide range of datasets. Moreover, we will use a simple mean imputer to handle the missing data. With in our use case is 92%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_valid_imputed = imputer.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression ---\n",
      "AUC: 0.5498\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "Accuracy: 0.8663\n",
      "F1 Score: 0.0000\n",
      "\n",
      "--- Decision Tree ---\n",
      "AUC: 0.5664\n",
      "Precision: 0.2440\n",
      "Recall: 0.2544\n",
      "Accuracy: 0.7950\n",
      "F1 Score: 0.2491\n",
      "\n",
      "[LightGBM] [Info] Number of positive: 936, number of negative: 6064\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.149757 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130494\n",
      "[LightGBM] [Info] Number of data points in the train set: 7000, number of used features: 2264\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.133714 -> initscore=-1.868509\n",
      "[LightGBM] [Info] Start training from score -1.868509\n",
      "--- LightGBM ---\n",
      "AUC: 0.7817\n",
      "Precision: 0.5347\n",
      "Recall: 0.1347\n",
      "Accuracy: 0.8687\n",
      "F1 Score: 0.2151\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=1),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=1),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=1)\n",
    "}\n",
    "\n",
    "# Function to evaluate and print metrics\n",
    "def evaluate_model(name, y_true, y_pred, y_pred_probs):\n",
    "    auc_score = roc_auc_score(y_true, y_pred_probs)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"AUC: {auc_score:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "# Iterate through models\n",
    "for name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(X_train_imputed, y_train)\n",
    "    \n",
    "    # Predict probabilities and classes\n",
    "    y_valid_pred_probs = model.predict_proba(X_valid_imputed)[:, 1]\n",
    "    y_valid_pred = model.predict(X_valid_imputed)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluate_model(name, y_valid, y_valid_pred, y_valid_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baselines Model Performance Overview\n",
    "- Logistic Regression\n",
    "  - Auc 0.54: Slightly better than random guessing (AUC of 0.5). Indicates poor discriminatory power.\n",
    "  - Precision: 0.0000: Model did not predict any positives, reflects difficulty in handling class imbalance.\n",
    "  - Recall: 0.0000: Model failed to capture any true positives.Indicates that the model is biased towards predicting the majority class (negative).\n",
    "  - Accuracy: 0.8663: High accuracy, but misleading due to class imbalance.\n",
    "\n",
    "**Logistic Regression struggles with the imbalanced dataset, failing to predict the minority class, resulting in poor performance across all metrics except accuracy (missleading due to class imbalance).**\n",
    "\n",
    "- Decision Tree\n",
    "  - AUC: 0.5664:Slightly better than Logistic Regression. Shows moderate discriminatory power.\n",
    "  - Precision: 0.2440: Indicates some ability to correctly predict positive cases.Affected by class imbalance.\n",
    "  - Recall: 0.2544: Slightly better at capturing true positives than Logistic Regression. Reflects some sensitivity to the minority class.\n",
    "  - Accuracy: 0.7950: Lower than Logistic Regression, but this is due to better handling of positive cases.\n",
    "  - F1 Score: 0.2491:Better balance between precision and recall.\n",
    "\n",
    "**The Decision Tree shows improved handling of the minority class compared to Logistic Regression, with better precision, recall, and F1 score, though it still struggles with the class imbalance.**\n",
    "\n",
    "- LightGBM\n",
    "  - AUC: 0.7817: Significantly better than Logistic Regression and Decision Tree.\n",
    "  - Precision: 0.5347: Highest precision among the models. Better at correctly predicting positive cases.\n",
    "  - Recall: 0.1347: Lower recall, indicating some challenges in capturing all true positives. Reflects trade-off between precision and recall.\n",
    "  - Accuracy: 0.8687: High accuracy, though  influenced by class imbalance.\n",
    "  - F1 Score: 0.2151: Reflects better  performance in handling the imbalanced dataset.\n",
    "\n",
    "**LightGBM outperforms the other models, especially in terms of AUC and precision, demonstrating better handling of the imbalanced dataset. The modelâ€™s ability to auto-tune parameters and handle missing data contributes to its superior performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of LightGBM Performance: Mean Imputation vs. Handling Missing Values Directly\n",
    "\n",
    "Finally, let's check how LightGBM performs with the preprocessed data with the missing values, instead of the mean impute.\n",
    "\n",
    "The lightGBM model accounts with a handling of missing values build in. It treats missing values as a separate category so:\n",
    "\n",
    "- Split Finding with Missing Values: Missing Value as a Separate Category: LightGBM treats missing values as a separate category when finding splits. During training, it can decide whether to assign missing values to the left or right side of a split.\n",
    "- Optimal Split Decision: LightGBM evaluates the best way to handle missing values by considering them during the split-finding process. This means it optimally decides where to place missing values to minimize the loss function.\n",
    "\n",
    "Reference: LightGBM's official documentation: LightGBM Handling Missing Values. https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#missing-value-handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 936, number of negative: 6064\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 124213\n",
      "[LightGBM] [Info] Number of data points in the train set: 7000, number of used features: 2478\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.133714 -> initscore=-1.868509\n",
      "[LightGBM] [Info] Start training from score -1.868509\n",
      "--- LightGBM ---\n",
      "AUC: 0.7884\n",
      "Precision: 0.5196\n",
      "Recall: 0.1322\n",
      "Accuracy: 0.8677\n",
      "F1 Score: 0.2107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LGBMClassifier(random_state=1)\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and classes\n",
    "y_valid_pred_probs = model.predict_proba(X_valid)[:, 1]\n",
    "y_valid_pred = model.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(name, y_valid, y_valid_pred, y_valid_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A marginally better ability to capture true positives when we include the mean imputation. However, we could not find any significant differences. The minimal difference indicates that both methods struggle equally with identifying all true positives."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
