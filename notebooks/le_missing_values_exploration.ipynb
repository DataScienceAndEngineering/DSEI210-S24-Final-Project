{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic and Baseline Model Analysis\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "Create a Random Model: This model will predict the target label based solely on the average percentage distribution of classes in the entire dataset.\n",
    "\n",
    "Build Baseline Models: Using the initially pre-processed data and missing values mean imputation, we will construct three baseline models:\n",
    "\n",
    "Logistic Regression\n",
    "Decision Tree\n",
    "LightGBM\n",
    "Objective\n",
    "\n",
    "The goal of this exercise is to establish baseline performance metrics, allowing us to compare these initial models with future models that incorporate more advanced transformations and methods. This comparison will help us determine if further enhancements lead to statistically significant improvements in accuracy and model stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import GroupKFold, cross_val_score, GridSearchCV, RandomizedSearchCV, train_test_split, KFold\n",
    "from sklearn.metrics import confusion_matrix, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, precision_score, recall_score, accuracy_score, f1_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Set the working directory\n",
    "os.chdir('c:/Users/laura/OneDrive/Documentos/Personal Documents/Universidad/DSE CCNY/Courses Semester 2/Applied ML/Final Project/machine-learning-dse-i210-final-project-credit-risk/notebooks')\n",
    "# Set data paths\n",
    "data_dir = 'c:/Users/laura/OneDrive/Documentos/Personal Documents/Universidad/DSE CCNY/Courses Semester 2/Applied ML/Final Project/new_aggs/new_aggs/'\n",
    "data_base = 'C:/Users/laura/OneDrive/Documentos/Personal Documents/Universidad/DSE CCNY/Courses Semester 2/Applied ML/Project_final/home-credit-credit-risk-model-stability/csv_files/train/train_base.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "For this baseline model we will use the union of the personal and non personal data after begin preprocessed. In this stage we will use just a set of that. This same code will be later run again with the complete dataset to get the final scores on the complete data. For the model based on the percentage, we pull the percentage of positive classes from the entire dataset 3% \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .pkl file for the personal data\n",
    "file1 = data_dir + 'df1.pkl'\n",
    "df = pd.read_pickle(file1)\n",
    "\n",
    "# Load the .pkl file for the non personal data\n",
    "file2 = data_dir + 'df2.pkl'\n",
    "df2 = pd.read_pickle(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joing the applicant personal data and the non personal one to get the complete dataset.\n",
    "df_full = df.merge(df2, on=['case_id', 'date_decision', 'WEEK_NUM'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2640)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm the dataset was properly merge by cheking the shape\n",
    "df_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>date_decision</th>\n",
       "      <th>MONTH_x</th>\n",
       "      <th>WEEK_NUM</th>\n",
       "      <th>target_x</th>\n",
       "      <th>empls_employedfrom_796D_distinct_x</th>\n",
       "      <th>empls_employedfrom_796D_min_year_x</th>\n",
       "      <th>empls_employedfrom_796D_min_month_x</th>\n",
       "      <th>empls_employedfrom_796D_min_day_x</th>\n",
       "      <th>empls_employedfrom_796D_max_year_x</th>\n",
       "      <th>...</th>\n",
       "      <th>pmts_pmtsoverdue_635A_median_y</th>\n",
       "      <th>pmts_dpdvalue_108P_sum_y</th>\n",
       "      <th>pmts_pmtsoverdue_635A_sum_y</th>\n",
       "      <th>pmts_date_1107D_distinct_y</th>\n",
       "      <th>pmts_date_1107D_min_year_y</th>\n",
       "      <th>pmts_date_1107D_min_month_y</th>\n",
       "      <th>pmts_date_1107D_min_day_y</th>\n",
       "      <th>pmts_date_1107D_max_year_y</th>\n",
       "      <th>pmts_date_1107D_max_month_y</th>\n",
       "      <th>pmts_date_1107D_max_day_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1488310</td>\n",
       "      <td>2019-08-14</td>\n",
       "      <td>201908</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13904</td>\n",
       "      <td>2019-05-06</td>\n",
       "      <td>201905</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>783503</td>\n",
       "      <td>2019-08-28</td>\n",
       "      <td>201908</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17986</td>\n",
       "      <td>2019-06-09</td>\n",
       "      <td>201906</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1400855</td>\n",
       "      <td>2019-06-13</td>\n",
       "      <td>201906</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 2639 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id date_decision  MONTH_x  WEEK_NUM  target_x  \\\n",
       "0  1488310    2019-08-14   201908        32         0   \n",
       "1    13904    2019-05-06   201905        17         0   \n",
       "2   783503    2019-08-28   201908        34         1   \n",
       "3    17986    2019-06-09   201906        22         1   \n",
       "4  1400855    2019-06-13   201906        23         0   \n",
       "\n",
       "   empls_employedfrom_796D_distinct_x  empls_employedfrom_796D_min_year_x  \\\n",
       "0                                 1.0                                 NaN   \n",
       "1                                 1.0                                 NaN   \n",
       "2                                 1.0                                 NaN   \n",
       "3                                 1.0                                 NaN   \n",
       "4                                 1.0                                 NaN   \n",
       "\n",
       "   empls_employedfrom_796D_min_month_x  empls_employedfrom_796D_min_day_x  \\\n",
       "0                                  NaN                                NaN   \n",
       "1                                  NaN                                NaN   \n",
       "2                                  NaN                                NaN   \n",
       "3                                  NaN                                NaN   \n",
       "4                                  NaN                                NaN   \n",
       "\n",
       "   empls_employedfrom_796D_max_year_x  ...  pmts_pmtsoverdue_635A_median_y  \\\n",
       "0                                 NaN  ...                             NaN   \n",
       "1                                 NaN  ...                             NaN   \n",
       "2                                 NaN  ...                             NaN   \n",
       "3                                 NaN  ...                             NaN   \n",
       "4                                 NaN  ...                             NaN   \n",
       "\n",
       "   pmts_dpdvalue_108P_sum_y  pmts_pmtsoverdue_635A_sum_y  \\\n",
       "0                       NaN                          NaN   \n",
       "1                       NaN                          NaN   \n",
       "2                       NaN                          NaN   \n",
       "3                       NaN                          NaN   \n",
       "4                       NaN                          NaN   \n",
       "\n",
       "   pmts_date_1107D_distinct_y  pmts_date_1107D_min_year_y  \\\n",
       "0                         NaN                         NaN   \n",
       "1                         NaN                         NaN   \n",
       "2                         NaN                         NaN   \n",
       "3                         NaN                         NaN   \n",
       "4                         NaN                         NaN   \n",
       "\n",
       "   pmts_date_1107D_min_month_y  pmts_date_1107D_min_day_y  \\\n",
       "0                          NaN                        NaN   \n",
       "1                          NaN                        NaN   \n",
       "2                          NaN                        NaN   \n",
       "3                          NaN                        NaN   \n",
       "4                          NaN                        NaN   \n",
       "\n",
       "   pmts_date_1107D_max_year_y  pmts_date_1107D_max_month_y  \\\n",
       "0                         NaN                          NaN   \n",
       "1                         NaN                          NaN   \n",
       "2                         NaN                          NaN   \n",
       "3                         NaN                          NaN   \n",
       "4                         NaN                          NaN   \n",
       "\n",
       "   pmts_date_1107D_max_day_y  \n",
       "0                        NaN  \n",
       "1                        NaN  \n",
       "2                        NaN  \n",
       "3                        NaN  \n",
       "4                        NaN  \n",
       "\n",
       "[5 rows x 2639 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the extra target column that appeared because of the join and it is just a duplicate of the target\n",
    "df_full.rename(columns={'target_x': 'target'}, inplace=True)\n",
    "\n",
    "# Drop target_y\n",
    "df_full.drop(columns=['target_y'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Data Cleaning\n",
    "\n",
    "We futher clean the data to be able to feed the previously stated predictors.\n",
    " - Boolean Columns:\n",
    "The boolean columns are filled with False where None is present, and then converted to boolean type using .astype(bool).\n",
    "- Object Columns:\n",
    "The object columns, which contain None, True, or False, are replaced with np.nan, 1.0, or 0.0 respectively, and converted to float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date_decision to timestamp\n",
    "df_full[\"date_decision\"] = pd.to_datetime(df_full[\"date_decision\"]).astype('int64') / 10**9\n",
    "\n",
    "# Get boolean columns from df_full\n",
    "bool_columns = df_full.select_dtypes(include=['bool']).columns.tolist()\n",
    "\n",
    "for col in bool_columns:\n",
    "    df_full[col] = df_full[col].fillna(False).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get object columns from df_full\n",
    "object_columns = df_full.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "for col in object_columns:\n",
    "    df_full[col] = df_full[col].replace({None: np.nan, True: 1.0, False: 0.0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Validation Split. \n",
    "\n",
    "We remove the target from the training datasets. We then further split the dataset into train and validation. For this split we do not apply startify to balanace the target, we are building the baseline which will not consider this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (7000, 2635), y_train: (7000,)\n",
      "Valid: (3000, 2635), y_valid: (3000,)\n"
     ]
    }
   ],
   "source": [
    "# Train and Validation split\n",
    "# Train and Validation split\n",
    "base = df_full[[\"case_id\", \"WEEK_NUM\", \"target\"]]\n",
    "X = df_full.drop(columns=[\"case_id\", \"WEEK_NUM\", \"target\"])\n",
    "y = df_full[\"target\"]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.7, random_state=1, stratify=y)\n",
    "\n",
    "# Prepare base_train and base_valid\n",
    "base_train = base.iloc[X_train.index]\n",
    "base_valid = base.iloc[X_valid.index]\n",
    "\n",
    "print(f\"Train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "print(f\"Valid: {X_valid.shape}, y_valid: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Model Predictor based on target percentage - Weighted Random Chance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of target = 1: 3.00%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the percentage of target = 1 in df_full\n",
    "percentage_target_1 = 3\n",
    "print(f\"Percentage of target = 1: {percentage_target_1:.2f}%\")\n",
    "\n",
    "\n",
    "def model_percentage(data, percentage):\n",
    "    random_numbers = np.random.rand(len(data))\n",
    "    predictions = (random_numbers < (percentage / 100)).astype(int)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Metrics Basic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hardcode the target positive class average to 3% which comes from the complete dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.5032\n",
      "Precision: 0.1553\n",
      "Recall: 0.0399\n",
      "Accuracy: 0.8427\n",
      "F1 Score: 0.0635\n"
     ]
    }
   ],
   "source": [
    "# Apply the model to base_valid\n",
    "y_valid_pred = model_percentage(base_valid, percentage_target_1)\n",
    "\n",
    "# Evaluate the model\n",
    "auc_score = roc_auc_score(y_valid, y_valid_pred)\n",
    "precision = precision_score(y_valid, y_valid_pred)\n",
    "recall = recall_score(y_valid, y_valid_pred)\n",
    "accuracy = accuracy_score(y_valid, y_valid_pred)\n",
    "f1 = f1_score(y_valid, y_valid_pred)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"AUC: {auc_score:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AUC score reflects the model's ability to distinguish between classes. A score closer to 0.5 indicates that the model performs no better than random guessing. Given that this is a random model based on the target's percentage, an AUC score near 0.5 is expected and confirms the model's lack of discriminatory power.\n",
    "\n",
    "Precision measures the proportion of true positives among all positive predictions. For a random model, precision is typically low as it doesn't have any mechanism to prioritize true positives over false positives.\n",
    "\n",
    "Recall measures the proportion of actual positives that are correctly identified by the model. A random model's recall is generally proportional to the actual prevalence of the positive class in the dataset. \n",
    "\n",
    "Accuracy measures the proportion of all correct predictions (both true positives and true negatives) out of the total predictions. For a random model, the accuracy is influenced by the class distribution in the dataset. An accuracy score that is close to the proportion of the majority class indicates that the model might be leaning towards predicting the majority class more often.\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall, providing a balance between the two. We see for this model that is low, reflecting the trade-off between precision and recall when there is no specific pattern in the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_stability(base, w_fallingrate=88.0, w_resstd=-0.5):\n",
    "    def safe_roc_auc_score(y_true, y_score):\n",
    "        \"\"\" Compute ROC AUC score only if there are two unique values in y_true \"\"\"\n",
    "        if len(set(y_true)) < 2:\n",
    "            return 0 \n",
    "        else:\n",
    "            return roc_auc_score(y_true, y_score)\n",
    "\n",
    "    gini_in_time = base.loc[:, [\"WEEK_NUM\", \"target\", \"score\"]]\\\n",
    "        .sort_values(\"WEEK_NUM\")\\\n",
    "        .groupby(\"WEEK_NUM\")[[\"target\", \"score\"]]\\\n",
    "        .apply(lambda x: 2 * safe_roc_auc_score(x[\"target\"], x[\"score\"]) - 1).tolist()\n",
    "\n",
    "    x = np.arange(len(gini_in_time))\n",
    "    y = gini_in_time\n",
    "    a, b = np.polyfit(x, y, 1)\n",
    "    y_hat = a * x + b\n",
    "    residuals = y - y_hat\n",
    "    res_std = np.std(residuals)\n",
    "    avg_gini = np.mean(gini_in_time)\n",
    "    return avg_gini + w_fallingrate * min(0, a) + w_resstd * res_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stability Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stability score on the train set is: -0.06003275294704073\n",
      "The stability score on the valid set is: -0.2564073926740489\n"
     ]
    }
   ],
   "source": [
    "for base, X in [(base_train, X_train), (base_valid, X_valid)]:\n",
    "    y_pred = model_percentage(X, percentage_target_1)\n",
    "    base[\"score\"] = y_pred\n",
    "\n",
    "stability_score_train = gini_stability(base_train)\n",
    "stability_score_valid = gini_stability(base_valid)\n",
    "\n",
    "print(f'The stability score on the train set is: {stability_score_train}') \n",
    "print(f'The stability score on the valid set is: {stability_score_valid}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "A more sofisticated Baseline we could use would be to train a Logistic Regressor, Decision Tree Classifier or LGBM Classier on their respective default parameters, which are pre-tuned to work reasonably well for a wide range of datasets. Moreover, we will use a simple mean imputer to handle the missing data. With in our use case is 92%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_imputed = imputer.fit_transform(X_train)\n",
    "X_valid_imputed = imputer.transform(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression ---\n",
      "AUC: 0.5498\n",
      "Precision: 0.0000\n",
      "Recall: 0.0000\n",
      "Accuracy: 0.8663\n",
      "F1 Score: 0.0000\n",
      "\n",
      "The stability score on the train set is: -0.02598690127861006\n",
      "The stability score on the valid set is: -0.1658323244940394\n",
      "--- Decision Tree ---\n",
      "AUC: 0.5664\n",
      "Precision: 0.2440\n",
      "Recall: 0.2544\n",
      "Accuracy: 0.7950\n",
      "F1 Score: 0.2491\n",
      "\n",
      "The stability score on the train set is: 0.811273401138279\n",
      "The stability score on the valid set is: -0.26950080972741763\n",
      "[LightGBM] [Info] Number of positive: 936, number of negative: 6064\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.170689 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 130494\n",
      "[LightGBM] [Info] Number of data points in the train set: 7000, number of used features: 2264\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.133714 -> initscore=-1.868509\n",
      "[LightGBM] [Info] Start training from score -1.868509\n",
      "--- LightGBM ---\n",
      "AUC: 0.7817\n",
      "Precision: 0.5347\n",
      "Recall: 0.1347\n",
      "Accuracy: 0.8687\n",
      "F1 Score: 0.2151\n",
      "\n",
      "The stability score on the train set is: 0.8142570932193411\n",
      "The stability score on the valid set is: 0.09537285986796756\n"
     ]
    }
   ],
   "source": [
    "# Define the models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=1),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=1),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=1)\n",
    "}\n",
    "\n",
    "# Function to evaluate and print metrics\n",
    "def evaluate_model(name, y_true, y_pred, y_pred_probs):\n",
    "    auc_score = roc_auc_score(y_true, y_pred_probs)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(f\"AUC: {auc_score:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\\n\")\n",
    "\n",
    "# Iterate through models\n",
    "for name, model in models.items():\n",
    "    # Fit the model\n",
    "    model.fit(X_train_imputed, y_train)\n",
    "    \n",
    "    # Predict probabilities and classes\n",
    "    y_valid_pred_probs = model.predict_proba(X_valid_imputed)[:, 1]\n",
    "    y_valid_pred = model.predict(X_valid_imputed)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    evaluate_model(name, y_valid, y_valid_pred, y_valid_pred_probs)\n",
    "\n",
    "    for base, X in [(base_train, X_train_imputed), (base_valid, X_valid_imputed)]:\n",
    "        y_pred = model.predict_proba(X)[:, 1]\n",
    "        base[\"score\"] = y_pred\n",
    "    \n",
    "    stability_score_train = gini_stability(base_train)\n",
    "    stability_score_valid = gini_stability(base_valid)\n",
    "\n",
    "    print(f'The stability score on the train set is: {stability_score_train}') \n",
    "    print(f'The stability score on the valid set is: {stability_score_valid}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baselines Model Performance Overview\n",
    "- Logistic Regression\n",
    "  - Auc 0.54: Slightly better than random guessing (AUC of 0.5). Indicates poor discriminatory power.\n",
    "  - Precision: 0.0000: Model did not predict any positives, reflects difficulty in handling class imbalance.\n",
    "  - Recall: 0.0000: Model failed to capture any true positives.Indicates that the model is biased towards predicting the majority class (negative).\n",
    "  - Accuracy: 0.8663: High accuracy, but misleading due to class imbalance.\n",
    "  - -0.16 gini score indicates that the model's predictions are no better than random chance.\n",
    "\n",
    "**Logistic Regression struggles with the imbalanced dataset, failing to predict the minority class, resulting in poor performance across all metrics except accuracy (missleading due to class imbalance).**\n",
    "\n",
    "- Decision Tree\n",
    "  - AUC: 0.5664:Slightly better than Logistic Regression. Shows moderate discriminatory power.\n",
    "  - Precision: 0.2440: Indicates some ability to correctly predict positive cases.Affected by class imbalance.\n",
    "  - Recall: 0.2544: Slightly better at capturing true positives than Logistic Regression. Reflects some sensitivity to the minority class.\n",
    "  - Accuracy: 0.7950: Lower than Logistic Regression, but this is due to better handling of positive cases.\n",
    "  - F1 Score: 0.2491:Better balance between precision and recall.\n",
    "  - -0.26 gini score indicates that the model's predictions are no better than random chance.\n",
    "\n",
    "**The Decision Tree shows improved handling of the minority class compared to Logistic Regression, with better precision, recall, and F1 score, though it still struggles with the class imbalance.**\n",
    "\n",
    "- LightGBM\n",
    "  - AUC: 0.7817: Significantly better than Logistic Regression and Decision Tree.\n",
    "  - Precision: 0.5347: Highest precision among the models. Better at correctly predicting positive cases.\n",
    "  - Recall: 0.1347: Lower recall, indicating some challenges in capturing all true positives. Reflects trade-off between precision and recall.\n",
    "  - Accuracy: 0.8687: High accuracy, though  influenced by class imbalance.\n",
    "  - F1 Score: 0.2151: Reflects better  performance in handling the imbalanced dataset.\n",
    "  - The stability score on the valid set is 0.095 which is closely to random chance but we see a slight improve. \n",
    "\n",
    "**LightGBM outperforms the other models, especially in terms of AUC and precision, demonstrating better handling of the imbalanced dataset. The modelâ€™s ability to auto-tune parameters and handle missing data contributes to its superior performance.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of LightGBM Performance: Mean Imputation vs. Handling Missing Values Directly\n",
    "\n",
    "Finally, let's check how LightGBM performs with the preprocessed data with the missing values, instead of the mean impute.\n",
    "\n",
    "The lightGBM model accounts with a handling of missing values build in. It treats missing values as a separate category so:\n",
    "\n",
    "- Split Finding with Missing Values: Missing Value as a Separate Category: LightGBM treats missing values as a separate category when finding splits. During training, it can decide whether to assign missing values to the left or right side of a split.\n",
    "- Optimal Split Decision: LightGBM evaluates the best way to handle missing values by considering them during the split-finding process. This means it optimally decides where to place missing values to minimize the loss function.\n",
    "\n",
    "Reference: LightGBM's official documentation: LightGBM Handling Missing Values. https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html#missing-value-handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 936, number of negative: 6064\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.218191 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 124213\n",
      "[LightGBM] [Info] Number of data points in the train set: 7000, number of used features: 2478\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.133714 -> initscore=-1.868509\n",
      "[LightGBM] [Info] Start training from score -1.868509\n",
      "--- LightGBM ---\n",
      "AUC: 0.7884\n",
      "Precision: 0.5196\n",
      "Recall: 0.1322\n",
      "Accuracy: 0.8677\n",
      "F1 Score: 0.2107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = LGBMClassifier(random_state=1)\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict probabilities and classes\n",
    "y_valid_pred_probs = model.predict_proba(X_valid)[:, 1]\n",
    "y_valid_pred = model.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(name, y_valid, y_valid_pred, y_valid_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A marginally better ability to capture true positives when we include the mean imputation. However, we could not find any significant differences. The minimal difference indicates that both methods struggle equally with identifying all true positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- Establishing Baselines:\n",
    "\n",
    "  - We utilized a random weighted model as the simplest baseline to understand the expected performance based purely on the dataset's class distribution.\n",
    "  - Additionally, we developed a more sophisticated baseline using the LightGBM model with default automated parameters.\n",
    "\n",
    "\n",
    "- Insights Gained:\n",
    "\n",
    "  - This exercise provided a clear understanding of the expected baseline performance given the current data distribution.\n",
    "  - The LightGBM model serves as a benchmark, demonstrating the performance achievable with minimal parameter tuning and preprocessing.\n",
    "\n",
    "- Next Steps:\n",
    "  - Although the current LightGBM model shows results that are only marginally better than random guessing, it sets a foundation for further improvement.\n",
    "  - Future efforts will focus on enhancing this baseline through advanced data preprocessing and transformation techniques aimed at significantly improving accuracy and model stability.\n",
    "\n",
    "- Objective:\n",
    "The goal is to surpass the performance of the LightGBM baseline by implementing more sophisticated data handling and model tuning strategies, aiming for a statistically significant improvement in the model's predictive capabilities. We will focus our effort in dealing with the target class inbalance, the missing values, and the great number of features. Moreover, we will investigate the relationship between regularization and the stability score. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
